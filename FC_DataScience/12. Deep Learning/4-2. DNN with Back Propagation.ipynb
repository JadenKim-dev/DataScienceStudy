{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a06392",
   "metadata": {},
   "source": [
    "## import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f7e9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6b18d",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba67bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A, B):\n",
    "    return np.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda3bb6",
   "metadata": {},
   "source": [
    "## Implement Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc5acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.last_o = 1  # 마지막 출력의 초기값 : 1\n",
    "    \n",
    "    def __call__(self, x): \n",
    "        self.last_o = 1.0 / (1.0 + np.exp(-x))  # 역전파 학습을 위해 마지막 출력을 기억\n",
    "        return self.last_o\n",
    "        \n",
    "    def grad(self):\n",
    "        return self.last_o * (1.0 - self.last_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef8ea5",
   "metadata": {},
   "source": [
    "## Mean Squared Error 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf727d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError:  \n",
    "    def __init__(self):\n",
    "        self.dh = 1   # 미분값을 기억\n",
    "        self.last_diff = 1   # h-y를 기억\n",
    "        \n",
    "    def __call__(self, h, y):\n",
    "        self.last_diff = h - y\n",
    "        return 1/2 * np.mean(np.square(self.last_diff))\n",
    "    \n",
    "    def grad(self):  # 1/2 * (h - y)^2 -grad->  h - y\n",
    "        return self.last_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605c29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, W, b, a_obj):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.a = a_obj()   # sigmoid를 class로 구현했으므로, 인스턴스화 해야 함  \n",
    "        # 각각의 Node마다 개별적으로 Sigmoid를 가지고 있음 -> 개별적으로 last_o 저장\n",
    "        \n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.dh = np.zeros_like(_t(self.W))\n",
    "        \n",
    "        self.last_x = np.zeros((self.W.shape[0]))\n",
    "        self.last_h = np.zeros((self.W.shape[1]))        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.last_x = x\n",
    "        self.last_h = _m(_t(self.W), x) + self.b\n",
    "        return self.a(self.last_h)\n",
    "        \n",
    "    def grad(self):  # dy/dh = W\n",
    "        return self.W * self.a.grad()\n",
    "        \n",
    "    def grad_W(self, dh):\n",
    "        grad = np.ones_like(self.W)\n",
    "        grad_a = self.a.grad()\n",
    "        for j in range(grad.shape[1]): # dy/dw = x  # 출력 뉴런 하나하나 마다 gradient를 구해서 넣음\n",
    "            grad[:, j] = dh[j] * grad_a[j] * self.last_x\n",
    "        return grad\n",
    "    \n",
    "    def grad_b(self, dh): # dy/db = 1\n",
    "        return dh * self.a.grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b1e93",
   "metadata": {},
   "source": [
    "## DNN with Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2139d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=Sigmoid):\n",
    "        def init_var(i, o):  # i, o : num of input/output variables\n",
    "            return np. random.normal(0.0, 0.01, (i,o)), np.zeros((o,))  # 초기 W, b 랜덤하게 할당\n",
    "        \n",
    "        self.sequence = list()\n",
    "        # First hidden layer\n",
    "        W, b = init_var(num_input, num_neuron)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "        \n",
    "        # hidden layers\n",
    "        for _ in range(hidden_depth):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Dense(W, b, activation))   \n",
    "        # Output layer\n",
    "        W, b = init_var(num_neuron, num_output)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "        \n",
    "    def __call__(self, x):  # Network에 저장된 sequence의 모든 layer를 이용하여 정방향으로 출력값 계산\n",
    "        for layer in self.sequence: \n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def calc_gradient(self, loss_obj):\n",
    "        loss_obj.dh = loss_obj.grad()\n",
    "        self.sequence.append(loss_obj)  # 임시로 넣어둔 것\n",
    "        \n",
    "        # back-pop loop\n",
    "        for i in range(len(self.sequence) - 1, 0, -1):  # 뒤 레이어부터 앞으로 순회\n",
    "            l1 = self.sequence[i]  # loss object가 됨 \n",
    "            l0 = self.sequence[i-1]\n",
    "            \n",
    "            l0.dh = _m(l0.grad(), l1.dh)\n",
    "            l0.dW = l0.grad_W(l1.dh)\n",
    "            l0.db = l0.grad_b(l1.dh)\n",
    "        \n",
    "        self.sequence.remove(loss_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b9ae0",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41febe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = loss_obj(network(x), y)  # Forward inference\n",
    "    network.calc_gradient(loss_obj) # Back-propagation\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11694a1e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344f2c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test loss 0.9140658861478284\n",
      "Epoch 11: Test loss 0.818844318331263\n",
      "Epoch 21: Test loss 0.7424649182629921\n",
      "Epoch 31: Test loss 0.684183665512473\n",
      "Epoch 41: Test loss 0.6401939924390871\n",
      "Epoch 51: Test loss 0.6066766346177853\n",
      "Epoch 61: Test loss 0.5806961736127831\n",
      "Epoch 71: Test loss 0.5601757887738992\n",
      "Epoch 81: Test loss 0.5436742871621133\n",
      "Epoch 91: Test loss 0.5301860918038411\n",
      "0.12703275680541992 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0.0, 1.0, (10, ))\n",
    "y = np.random.normal(0.0, 1.0, (2, ))\n",
    "\n",
    "t = time.time()\n",
    "dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=Sigmoid)\n",
    "loss_obj = MeanSquaredError()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, loss_obj, 0.01)\n",
    "    if epoch % 10 == 1:\n",
    "        print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
