{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9f5381",
   "metadata": {},
   "source": [
    "## import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f8e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8367da0",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862cd2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "\n",
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A, B):  # matrix multiplication\n",
    "    return np.matmul(A, B)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def mean_squared_error(h, y):\n",
    "    return 1/2*np.mean(np.square(h-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b673d",
   "metadata": {},
   "source": [
    "## implement Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a945789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:  # Fully connected layer\n",
    "    def __init__(self, W, b, a):  # a : activation function\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.a = a\n",
    "        \n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def __call__(self, x):  # 정방향 계산\n",
    "        return self.a(_m(_t(self.W), x) + self.b)  # matmul((ixo)T, ix1) + ox1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d224a",
   "metadata": {},
   "source": [
    "## DNN without Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86809da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):\n",
    "        def init_var(i, o):  # i, o : num of input/output variables\n",
    "            return np. random.normal(0.0, 0.01, (i,o)), np.zeros((o,))  # 초기 W, b 랜덤하게 할당\n",
    "        \n",
    "        self.sequence = list()\n",
    "        # First hidden layer\n",
    "        W, b = init_var(num_input, num_neuron)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "        \n",
    "        # hidden layers\n",
    "        for _ in range(hidden_depth-1):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Dense(W, b, activation))   \n",
    "        # Output layer\n",
    "        W, b = init_var(num_neuron, num_output)\n",
    "        self.sequence.append(Dense(W, b, activation))\n",
    "        \n",
    "    def __call__(self, x):  # Network에 저장된 sequence의 모든 layer를 이용하여 정방향으로 출력값 계산\n",
    "        for layer in self.sequence: \n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def calc_gradient(self, x, y, loss_func):  # calculate numerical gradient\n",
    "        \n",
    "        def get_new_sequence(layer_index, new_layer): # layer_index : 바꾸려는 layer의 idx,  # new_layer : 새로운 layer\n",
    "            new_sequence = list()\n",
    "            for i, layer in enumerate(self.sequence):\n",
    "                if i == layer_index:  # 바꾸려고 하는 index에 도달하면\n",
    "                    new_sequence.append(new_layer)\n",
    "                else:  # 그 외에는 기존 layer 사용\n",
    "                    new_sequence.append(layer)\n",
    "            return new_sequence\n",
    "        \n",
    "        # 지정한 sequence의 모든 layer를 이용하여 정방향으로 출력값 계산\n",
    "        def eval_sequence(x, sequence):\n",
    "            for layer in sequence:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "        \n",
    "        loss = loss_func(self(x), y)   # 기준이 되는, 초기 parameter를 통한 loss   # self(x)  : __call__(x)\n",
    "        \n",
    "        for layer_id, layer in enumerate(self.sequence):  # 모든 layer 순회\n",
    "    \n",
    "            # 모든 parameter w, b에 대해서 순회를 돌면서 numerical gradient 계산\n",
    "            for w_i, w in enumerate(layer.W):  # 행에 대한 iteration\n",
    "                for w_j, ww in enumerate(w):  # 열에 대한 iteration\n",
    "                    W = np.copy(layer.W)\n",
    "                    W[w_i][w_j] = ww + epsilon\n",
    "                    \n",
    "                    new_layer = Dense(W, layer.b, layer.a)\n",
    "                    new_seq = get_new_sequence(layer_id, new_layer)\n",
    "                    h = eval_sequence(x, new_seq)\n",
    "                    \n",
    "                    num_grad = (loss_func(h, y) - loss) / epsilon  # (f(x+eps) - f(x)) / eps\n",
    "                    layer.dW[w_i][w_j] = num_grad\n",
    "                \n",
    "            for b_i, bb in enumerate(layer.b):\n",
    "                b = np.copy(layer.b)\n",
    "                b[b_i] = bb + epsilon\n",
    "\n",
    "                new_layer = Dense(layer.W, b, layer.a)\n",
    "                new_seq = get_new_sequence(layer_id, new_layer)\n",
    "                h = eval_sequence(x, new_seq)\n",
    "\n",
    "                num_grad = (loss_func(h, y) - loss) / epsilon  # (f(x+eps) - f(x)) / eps\n",
    "                layer.db[b_i] = num_grad\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17e6b2",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b93967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = network.calc_gradient(x, y, loss_obj) # 각각의 layer의 모든 parameter b, w에 대해 gradient 계산\n",
    "    for layer in network.sequence:   # update W, b\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss  # 학습 과정에서의 loss를 return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f09a0",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fdf1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test loss 1.0238182679908203\n",
      "Epoch 11: Test loss 0.9680340202563503\n",
      "Epoch 21: Test loss 0.9168151196164134\n",
      "Epoch 31: Test loss 0.8709576028775313\n",
      "Epoch 41: Test loss 0.8306145139363748\n",
      "Epoch 51: Test loss 0.7954961156707366\n",
      "Epoch 61: Test loss 0.7650786348562719\n",
      "Epoch 71: Test loss 0.7387589991737111\n",
      "Epoch 81: Test loss 0.7159467342022199\n",
      "Epoch 91: Test loss 0.696108198465763\n",
      "66.88063025474548 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0.0, 1.0, (10, ))\n",
    "y = np.random.normal(0.0, 1.0, (2, ))\n",
    "\n",
    "dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)\n",
    "\n",
    "t = time.time()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01)\n",
    "    if epoch % 10 == 1:\n",
    "        print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
